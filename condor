#!/usr/bin/env python3

import argparse
import getpass
import htcondor
import os
import re
import sys
import stat

from datetime import datetime, timedelta

JobStatus = [
    "NONE",
    "IDLE",
    "RUNNING",
    "REMOVED",
    "COMPLETED",
    "HELD",
    "TRANSFERRING_OUTPUT",
    "SUSPENDED",
    "JOB_STATUS_MAX"
]

# Some global objects we'll need everywhere
schedd = None

def print_help(stream=sys.stderr):
    help_msg = """Usage: {0} <object> <action> [<resource>] [<option1> <option2> ...]

"""
    stream.write(help_msg.format(sys.argv[0]))


def parse_args():

    # The only arguments that are acceptable are
    # <this> <object> <action>
    # <this> <object> <action> <target>
    # <this> <object> <action> <resource> [<option1> <option2> ...]

    if len(sys.argv) < 3:
        print_help()
        sys.exit(1)

    parser = argparse.ArgumentParser()
    command = {}
    options = {}

    # Command arguments: up to 3 unflagged arguments at the beginning
    parser.add_argument("command", nargs="*")
    # Options arguments: optional flagged arguments following the command args
    parser.add_argument("--resource", help="Type of compute resource")
    parser.add_argument("--runtime", type=int, action="store", help="Runtime for provisioned Slurm glideins (seconds)")
    parser.add_argument("--node_count", type=int, action="store", help="Number of Slurm nodes to provision")
    parser.add_argument("--email", type=str, action="store", help="Email address for notifications")
    parser.add_argument("--schedd", help="Address to remote schedd to query")

    args = parser.parse_args()

    command_object = args.command[0]
    try:
        command["object"] = command_object.lower()
    except:
        print(f"Error: Object must be a string")
        sys.exit(1)

    command_action = args.command[1]
    try:
        command["action"] = command_action.lower()
    except:
        print(f"Error: Action must be a string")
        sys.exit(1)

    if len(args.command) > 2:
        command["target"] = args.command[2]

    if args.resource is not None:
        options["resource"] = args.resource.lower()

    if args.runtime is not None:
        try:
            options["runtime"] = int(args.runtime)
        except:
            print(f"Error: The --runtime argument must take a numeric value")
            sys.exit(1)
    
    if args.node_count is not None:
        try:
            options["node_count"] = int(args.node_count)
        except:
            print(f"Error: The --node_count argument must take a numeric value")
            sys.exit(1)

    return {
        "command": command,
        "options": options
    }


class DAGMan:
    """
    A :class:`DAGMan` holds all operations related to DAGMan jobs
    """

    @staticmethod
    def get_files(dagman_id):
        """
        Retrieve the filenames of a DAGs output and event logs based on 
        DAGMan cluster id
        """
    
        dag, log, out = None, None, None

        env = schedd.query(
            constraint=f"ClusterId == {dagman_id}",
            projection=["Env"],
        )

        if env:
            env = dict(item.split("=") for item in env[0]["Env"].split(";"))
            out = env["_CONDOR_DAGMAN_LOG"]
            log = out.replace(".dagman.out", ".nodes.log")
            dag = out.replace(".dagman.out", "")

        return dag, out, log

    @staticmethod
    def write_slurm_dag(jobfile, runtime, node_count, email="user@domain.com"):

        sendmail_sh = f"""#!/bin/sh

#sendmail {email}
"""

        sendmail_sh_file = open("sendmail.sh", "w")
        sendmail_sh_file.write(sendmail_sh)
        sendmail_sh_file.close()
        st = os.stat("sendmail.sh")
        os.chmod("sendmail.sh", st.st_mode | stat.S_IEXEC)

        slurm_config = "DAGMAN_USE_CONDOR_SUBMIT = False"
        slurm_config_file = open("slurm_submit.config", "w")
        slurm_config_file.write(slurm_config)
        slurm_config_file.close()

        slurm_dag = f"""JOB A {{
    executable = sendmail.sh
    universe = local
    output = job-A-email.$(cluster).$(process).out
    request_disk = 10M
}}
JOB B {{
    annex_runtime = {runtime}
    annex_node_count = {node_count}
    annex_name = {getpass.getuser()}-test
    annex_user = {getpass.getuser()}
    universe=grid
    grid_resource=batch slurm hpclogin1.chtc.wisc.edu
    transfer_executable=false
    executable=/home/jfrey/hobblein/hobblein_remote.sh
    # args: <node count> <run time> <annex name> <user>
    arguments=$(annex_node_count) $(annex_runtime) $(annex_name) $(annex_user)
    +NodeNumber=$(annex_node_count)
    #+HostNumber=$(annex_node_count)
    +BatchRuntime=$(annex_runtime)
    #+WholeNodes=true
    output = job-B.$(Cluster).$(Process).out
    error = job-B.$(Cluster).$(Process).err
    log = annex.log
    request_disk = 30
    notification=NEVER
}}
JOB C {jobfile}
JOB D {{
    executable = sendmail.sh
    universe = local
    output = job-D-email.$(cluster).$(process).out
    request_disk = 10M
}}

PARENT A CHILD B C
PARENT B C CHILD D

CONFIG slurm_submit.config

VARS C Requirements="(Facility == \\\"CHTC_Slurm\\\")"
VARS C +MayUseSlurm="True"
VARS C +WantFlocking="True"
"""

        dag_file = open("slurm_submit.dag", "w")
        dag_file.write(slurm_dag)
        dag_file.close()

    @staticmethod
    def write_ec2_dag(jobfile, runtime, node_count, email="user@domain.com"):

        sendmail_sh = f"""#!/bin/sh

#sendmail {email}
"""

        sendmail_sh_file = open("sendmail.sh", "w")
        sendmail_sh_file.write(sendmail_sh)
        sendmail_sh_file.close()
        st = os.stat("sendmail.sh")
        os.chmod("sendmail.sh", st.st_mode | stat.S_IEXEC)

        ec2_config = "DAGMAN_USE_CONDOR_SUBMIT = False"
        ec2_config_file = open("ec2_submit.config", "w")
        ec2_config_file.write(ec2_config)
        ec2_config_file.close()

        ec2_dag = f"""JOB A {{
    executable = sendmail.sh
    universe = local
    output = job-A-email.$(cluster).$(process).out
    request_disk = 10M
}}
JOB B {{
    executable = condor_annex
    arguments = -count {node_count} -duration {runtime} -annex-name EC2Annex
    output = job-B-ec2_annex.$(Cluster).$(Process).out
    error = job-B-ec2_annex.$(Cluster).$(Process).err
    log = ec2_annex.log
}}
JOB C {jobfile}
JOB D {{
    executable = sendmail.sh
    universe = local
    output = job-D-email.$(cluster).$(process).out
    request_disk = 10M
}}

PARENT A CHILD B C
PARENT B C CHILD D

CONFIG ec2_submit.config
"""

        dag_file = open("ec2_submit.dag", "w")
        dag_file.write(ec2_dag)
        dag_file.close()


class Job:
    """
    A :class:`Job` holds all operations related to HTCondor jobs
    """

    @staticmethod
    def submit(file, options=None):

        if "resource" not in options:

            # If no resource specified, submit job to the local schedd
            try:
                submit_file = open(file)
            except:
                print(f"Error: could not read file {file}")
                sys.exit(1)
            submit_data = submit_file.read()
            submit_file.close()
            submit_description = htcondor.Submit(submit_data)

            # The Job class can only submit a single job at a time
            submit_qargs = submit_description.getQArgs()
            if submit_qargs != "" and submit_qargs != "1":
                print("Error: can only submit one job at a time. See the job-set syntax for submitting multiple jobs.")
                sys.exit(1)

            with schedd.transaction() as txn:
                try:
                    cluster_id = submit_description.queue(txn, 1)
                    print(f"Job {cluster_id} was submitted.")
                except Exception as error:
                    print(f"Error submitting job: f{error.what()}")
                    sys.exit(1)

        elif options["resource"] == "slurm":

            if "runtime" not in options:
                print("Error: Slurm resources must specify a --runtime argument")
                sys.exit(1)
            if "node_count" not in options:
                print("Error: Slurm resources must specify a --node_count argument")
                sys.exit(1)

            # Delete any leftover files
            # TODO: Find a safer way to do this, avoid deleting active .nodes.log files
            os.system("rm -rf slurm_submit.dag*")

            DAGMan.write_slurm_dag(file, options["runtime"], options["node_count"])
            submit_description = htcondor.Submit.from_dag("slurm_submit.dag")
            submit_description["+ResourceType"] = "\"Slurm\""
            
            # The Job class can only submit a single job at a time
            submit_qargs = submit_description.getQArgs()
            if submit_qargs != "" and submit_qargs != "1":
                print("Error: can only submit one job at a time. See the job-set syntax for submitting multiple jobs.")
                sys.exit(1)

            with schedd.transaction() as txn:
                try:
                    cluster_id = submit_description.queue(txn, 1)
                    print(f"Job {cluster_id} was submitted.")
                except Exception as error:
                    print(f"Error submitting job: f{error.what()}")
                    sys.exit(1)

        elif options["resource"] == "ec2":

            if "runtime" not in options:
                print("Error: EC2 resources must specify a --runtime argument")
                sys.exit(1)
            if "node_count" not in options:
                print("Error: EC2 resources must specify a --node_count argument")
                sys.exit(1)

            # Delete any leftover files
            # TODO: Find a safer way to do this, avoid deleting active .nodes.log files
            os.system("rm -rf ec2_submit.dag*")

            DAGMan.write_ec2_dag(file, options["runtime"], options["node_count"])
            submit_description = htcondor.Submit.from_dag("ec2_submit.dag")
            submit_description["+ResourceType"] = "\"EC2\""

            # The Job class can only submit a single job at a time
            submit_qargs = submit_description.getQArgs()
            if submit_qargs != "" and submit_qargs != "1":
                print("Error: can only submit one job at a time. See the job-set syntax for submitting multiple jobs.")
                sys.exit(1)

            with schedd.transaction() as txn:
                try:
                    cluster_id = submit_description.queue(txn, 1)
                    print(f"Job {cluster_id} was submitted.")
                except Exception as error:
                    print(f"Error submitting job: f{error.what()}")
                    sys.exit(1)


    @staticmethod
    def status(id, options=None):
        """
        Displays the status of a job
        """

        job = None
        job_status = "IDLE"
        resource_type = "htcondor"

        try:
            job = schedd.query(
                constraint=f"ClusterId == {id}",
                projection=["JobStartDate", "JobStatus", "LastVacateTime", "ResourceType"]
            )
        except IndexError:
            print(f"No job found for ID {id}.")
            sys.exit(0)
        except:
            print(f"Error looking up job status: {sys.exc_info()[0]}")
            sys.exit(1)

        if len(job) == 0:
            print(f"No job found for ID {id}.")
            sys.exit(0)
            
        if "ResourceType" in job[0]:
            resource_type = job[0]["ResourceType"].lower()

        # Now, produce job status based on the resource type
        if resource_type == "htcondor":
            if JobStatus[job[0]['JobStatus']] is "RUNNING":
                job_running_time = datetime.now() - datetime.fromtimestamp(job[0]["JobStartDate"])
                print(f"Job is {JobStatus[job[0]['JobStatus']]} since {round(job_running_time.seconds/3600)}h{round(job_running_time.seconds/60)}m{(job_running_time.seconds%60)}s")
            elif JobStatus[job[0]['JobStatus']] is "HELD":
                job_held_time = datetime.now() - datetime.fromtimestamp(job[0]["LastVacateTime"])
                print(f"Job is {JobStatus[job[0]['JobStatus']]} since {round(job_held_time.seconds/3600)}h{round(job_held_time.seconds/60)}m{(job_held_time.seconds%60)}s")
            else:
                print(f"Job is {JobStatus[job[0]['JobStatus']]}")

        # Jobs running on provisioned Slurm resources need to retrieve
        # additional information from the provisioning DAGMan log
        elif resource_type == "slurm":

            # Variables specific to jobs running on Slurm clusters
            jobs_running = 0
            job_started_time = None
            provisioner_cluster_id = None
            provisioner_job_submitted_time = None
            slurm_cluster_id = None
            slurm_nodes_requested = None
            slurm_runtime = None

            dagman_dag, dagman_out, dagman_log = DAGMan.get_files(id)

            if dagman_dag is None:
                print(f"No Slurm job found for ID {id}.")
                sys.exit(0)

            # Parse the .dag file to retrieve some user input values
            dagman_dag_file = open(dagman_dag, "r")
            for line in dagman_dag_file.readlines():
                if "annex_node_count =" in line:
                    slurm_nodes_requested = line.split("=")[1].strip()
                if "annex_runtime =" in line:
                    slurm_runtime = int(line.split("=")[1].strip())
            
            # Parse the DAGMan event log for useful information
            dagman_events = htcondor.JobEventLog(dagman_log)
            for event in dagman_events.events(0):
                if "LogNotes" in event.keys() and event["LogNotes"] == "DAG Node: B":
                    provisioner_cluster_id = event.cluster
                    provisioner_job_submitted_time = datetime.fromtimestamp(event.timestamp)
                    job_status = "PROVISIONING REQUEST PENDING"
                elif "LogNotes" in event.keys() and event["LogNotes"] == "DAG Node: C":
                    slurm_cluster_id = event.cluster
                elif event.cluster == slurm_cluster_id and event.type == htcondor.JobEventType.EXECUTE:
                    job_status = "RUNNING"
                    jobs_running += 1
                    if job_started_time is None:
                        job_started_time = datetime.fromtimestamp(event.timestamp)
                elif event.cluster == slurm_cluster_id and event.type == htcondor.JobEventType.JOB_TERMINATED:
                    jobs_running -= 1
                    if jobs_running == 0:
                        job_status = "COMPLETE"
                elif event.type == htcondor.JobEventType.JOB_HELD or event.type == htcondor.JobEventType.EXECUTABLE_ERROR:
                    job_status = "ERROR"

            # Now that we have all the information we want, display it
            current_time = datetime.now()
            time_diff = None
            if job_status is "PROVISIONING REQUEST PENDING":
                time_diff = current_time - provisioner_job_submitted_time
            elif job_status is "RUNNING":
                time_diff = current_time - job_started_time

            print(f"Job is {job_status}", end='')
            if time_diff is not None:
                print(f" since {round(time_diff.seconds/60)}m{(time_diff.seconds%60)}s")
            else:
                print("")

        else:
            print("Error: The 'job status' command does not support {resource_type} resources.")
            sys.exit(1)


    @staticmethod
    def resources(id, options=None):
        """
        Displays the resources used by a specified job
        """

        # If no resource specified, assume job is running on local pool
        if "resource" not in options:
            try:
                job = schedd.query(
                    constraint=f"ClusterId == {id}",
                    projection=["RemoteHost"]
                )
            except IndexError:
                print(f"No jobs found for ID {id}.")
                sys.exit(0)
            except:
                print(f"Unable to look up job resources")
                sys.exit(1)
                
            if len(job) == 0:
                print(f"No jobs found for ID {id}.")
                sys.exit(0)
            
            # TODO: Make this work correctly for jobs that havne't started running yet 
            job_host = job[0]["RemoteHost"]
            print(f"Job is using resource {job_host}")

        # Jobs running on provisioned Slurm resources need to retrieve
        # additional information from the provisioning DAGMan log
        elif options["resource"] == "slurm":

            # Internal variables
            dagman_cluster_id = None
            provisioner_cluster_id = None
            slurm_cluster_id = None
        
            # User-facing variables (all values set below are default/initial state)
            provisioner_job_submitted_time = None
            provisioner_job_scheduled_end_time = None
            job_status = "NOT SUBMITTED"
            job_started_time = None
            jobs_running = 0
            slurm_nodes_requested = None
            slurm_runtime = None

            dagman_dag, dagman_out, dagman_log = DAGMan.get_files(id)

            if dagman_dag is None:
                print(f"No Slurm job found for ID {id}.")
                sys.exit(0)

            # Parse the .dag file to retrieve some user input values
            dagman_dag_file = open(dagman_dag, "r")
            for line in dagman_dag_file.readlines():
                if "annex_node_count =" in line:
                    slurm_nodes_requested = line.split("=")[1].strip()
                if "annex_runtime =" in line:
                    slurm_runtime = int(line.split("=")[1].strip())

            # Parse the DAGMan event log for useful information
            dagman_events = htcondor.JobEventLog(dagman_log)
            for event in dagman_events.events(0):
                if "LogNotes" in event.keys() and event["LogNotes"] == "DAG Node: B":
                    provisioner_cluster_id = event.cluster
                    provisioner_job_submitted_time = datetime.fromtimestamp(event.timestamp)
                    provisioner_job_scheduled_end_time = datetime.fromtimestamp(event.timestamp + slurm_runtime)
                    job_status = "PROVISIONING REQUEST PENDING"
                if event.cluster == provisioner_cluster_id and event.type == htcondor.JobEventType.EXECUTE:
                    provisioner_job_started_time = datetime.fromtimestamp(event.timestamp)
                    provisioner_job_scheduled_end_time = datetime.fromtimestamp(event.timestamp + slurm_runtime)
                if "LogNotes" in event.keys() and event["LogNotes"] == "DAG Node: C":
                    slurm_cluster_id = event.cluster
                    job_started_time = datetime.fromtimestamp(event.timestamp)
                if event.cluster == slurm_cluster_id and event.type == htcondor.JobEventType.EXECUTE:
                    job_status = "RUNNING"
                    jobs_running += 1
                if event.cluster == slurm_cluster_id and (event.type == htcondor.JobEventType.JOB_TERMINATED or event.type == htcondor.JobEventType.JOB_EVICTED):
                    jobs_running -= 1
                    if jobs_running == 0:
                        job_status = "COMPLETE"
                if event.type == htcondor.JobEventType.JOB_HELD or event.type == htcondor.JobEventType.EXECUTABLE_ERROR:
                    job_status = "ERROR"

            # Now that we have all the information we want, display it
            if job_status is "PROVISIONING REQUEST PENDING":
                print(f"Job is still waiting for {slurm_nodes_requested} Slurm nodes to provision")
            elif job_status is "RUNNING":
                print(f"Job is running on {jobs_running}/{slurm_nodes_requested} requested Slurm nodes")
            elif job_status is "ERROR":
                print(f"An error occurred provisioning Slurm resources")

            # Show information about time remaining
            if job_status is "RUNNING" or job_status is "COMPLETE":
                current_time = datetime.now()
                if current_time < provisioner_job_scheduled_end_time:
                    time_diff = provisioner_job_scheduled_end_time - current_time
                    print(f"Slurm resources are reserved for another {round(time_diff.seconds/60)}m{(time_diff.seconds%60)}s")
                else:
                    time_diff = current_time - provisioner_job_scheduled_end_time
                    print(f"Slurm resources were terminated since {round(time_diff.seconds/60)}m{(time_diff.seconds%60)}s")


def main():

    # Parse arguments and set default values if undeclared
    try:
        args = parse_args()
    except Exception as err:
        print(f"Failed to parse arguments: {err}", file=sys.stderr)

    # Assume any error checking on the inputs happens during parsing
    command = args["command"]
    options = args["options"]

    # Set up some globally-needed things
    global schedd
    try:
        schedd = htcondor.Schedd()
    except:
        print(f"Could not access local schedd. This tool must be a run from a submit machine, or specify a remote submit host with the --schedd=schedd.hostname flag.")
        sys.exit(1)

    # Figure out what the user is asking for and show it
    if command["object"] == "job":
        if command["action"] == "submit":
            Job.submit(command["target"], options)
        elif command["action"] == "status":
            Job.status(command["target"], options)
        elif command["action"] == "resources":
            Job.resources(command["target"], options)
        else:
            print(f"Error: The '{command['action']}' action is not supported for job objects")
    else:
        print(f"Error: '{command['object']}' is not a valid command object") 

    # All done
    sys.exit(0)


if __name__ == "__main__":
    main()
