#!/usr/bin/env python3

import argparse
import htcondor
import os
import re
import sys

from datetime import datetime, timedelta

JobStatus = [
    "NONE",
    "IDLE",
    "RUNNING",
    "REMOVED",
    "COMPLETED",
    "HELD",
    "TRANSFERRING_OUTPUT",
    "SUSPENDED",
    "JOB_STATUS_MAX"
]

# Some global objects we'll need everywhere
schedd = None

def print_help(stream=sys.stderr):
    help_msg = """Usage: {0} <object> <action> [<resource>] [<option1> <option2> ...]

"""
    stream.write(help_msg.format(sys.argv[0]))


def parse_args():

    # The only arguments that are acceptable are
    # <this> <object> <action>
    # <this> <object> <action> <target>
    # <this> <object> <action> <resource> [<option1> <option2> ...]

    if len(sys.argv) < 3:
        print_help()
        sys.exit(1)

    parser = argparse.ArgumentParser()
    command = {}
    options = {}

    # Command arguments: up to 3 unflagged arguments at the beginning
    parser.add_argument("command", nargs="*")
    # Options arguments: optional flagged arguments following the command args
    parser.add_argument("--resource", help="Type of compute resource")
    parser.add_argument("--schedd", help="Address to remote schedd to query")

    args = parser.parse_args()

    command_object = args.command[0]
    try:
        command["object"] = command_object.lower()
    except:
        print(f"ERROR: Object must be a string")
        sys.exit(1)

    command_action = args.command[1]
    try:
        command["action"] = command_action.lower()
    except:
        print(f"ERROR: Action must be a string")
        sys.exit(1)

    if len(args.command) > 2:
        command["target"] = args.command[2]

    if args.resource is not None:
        options["resource"] = args.resource.lower()

    return {
        "command": command,
        "options": options
    }


class DAGMan:
    """
    A :class:`DAGMan` holds all operations related to DAGMan jobs
    """

    @staticmethod
    def get_files(dagman_id):
        """
        Retrieve the filenames of a DAGs output and event logs based on 
        DAGMan cluster id
        """
    
        dag, log, out = None, None, None

        env = schedd.query(
            constraint=f"ClusterId == {dagman_id}",
            projection=["Env"],
        )

        if env:
            env = dict(item.split("=") for item in env[0]["Env"].split(";"))
            out = env["_CONDOR_DAGMAN_LOG"]
            log = out.replace(".dagman.out", ".nodes.log")
            dag = out.replace(".dagman.out", "")

        return dag, out, log


class Job:
    """
    A :class:`Job` holds all operations related to HTCondor jobs
    """

    @staticmethod
    def submit(file, options=None):

        # If no resource specified, submit job to the local schedd
        try:
            submit_file = open(file)
        except:
            print(f"Could not read file: {file}")
            sys.exit(1)
        submit_data = submit_file.read()
        submit_file.close()

        submit_description = htcondor.Submit(submit_data)
        with schedd.transaction() as txn:
            try:
                cluster_id = submit_description.queue(txn, 1)
                print(f"Job(s) submitted to cluster {cluster_id}")
            except Exception as error:
                print(f"Error submitting job: f{error.what()}")
                sys.exit(1)


    @staticmethod
    def status(id, options=None):
        """
        Displays the status of a job
        """

        job = None

        # If no resource specified, assume job is running on local pool
        if "resource" not in options:
            try:
                job = schedd.query(
                    constraint=f"ClusterId == {id}",
                    projection=["JobStartDate", "JobStatus", "LastVacateTime"]
                )
                print(f"Job is {JobStatus[job[0]['JobStatus']]}", end="")
            except IndexError:
                print(f"No jobs found for cluster ID {id}.")
                sys.exit(0)
            except:
                print(f"Error looking up job status: {sys.exc_info()[0]}")
                sys.exit(1)

            if JobStatus[job[0]['JobStatus']] is "RUNNING":
                job_running_time = datetime.now() - datetime.fromtimestamp(job[0]["JobStartDate"])
                print(f" since {round(job_running_time.seconds/3600)}h{round(job_running_time.seconds/60)}m{(job_running_time.seconds%60)}s")
            elif JobStatus[job[0]['JobStatus']] is "HELD":
                job_held_time = datetime.now() - datetime.fromtimestamp(job[0]["LastVacateTime"])
                print(f" since {round(job_held_time.seconds/3600)}h{round(job_held_time.seconds/60)}m{(job_held_time.seconds%60)}s")
            else:
                print("")

        # Jobs running on provisioned Slurm resources need to retrieve
        # additional information from the provisioning DAGMan log
        elif options["resource"] == "slurm":

            # Variables specific to jobs running on Slurm clusters
            jobs_running = 0
            provisioner_cluster_id = None
            provisioner_job_started_time = None
            slurm_cluster_id = None
            slurm_nodes_requested = None
            slurm_runtime = None

            dagman_dag, dagman_out, dagman_log = DAGMan.get_files(id)

            if dagman_dag is None:
                print(f"No Slurm jobs found for cluster ID {id}.")
                sys.exit(0)

            # Parse the .dag file to retrieve some user input values
            dagman_dag_file = open(dagman_dag, "r")
            for line in dagman_dag_file.readlines():
                if "annex_node_count =" in line:
                    slurm_nodes_requested = line.split("=")[1].strip()
                if "annex_runtime =" in line:
                    slurm_runtime = int(line.split("=")[1].strip())
            
            # Parse the DAGMan event log for useful information
            dagman_events = htcondor.JobEventLog(dagman_log)
            for event in dagman_events.events(0):
                if "LogNotes" in event.keys() and event["LogNotes"] == "DAG Node: B":
                    provisioner_cluster_id = event.cluster
                    provisioner_job_started_time = datetime.fromtimestamp(event.timestamp)
                    job_status = "PROVISIONING REQUEST PENDING"
                if "LogNotes" in event.keys() and event["LogNotes"] == "DAG Node: C":
                    slurm_cluster_id = event.cluster
                    if job_started_time is None:
                        job_started_time = datetime.fromtimestamp(event.timestamp)
                if event.cluster == slurm_cluster_id and event.type == htcondor.JobEventType.EXECUTE:
                    job_status = "RUNNING"
                    jobs_running += 1
                if event.cluster == slurm_cluster_id and event.type == htcondor.JobEventType.JOB_TERMINATED:
                    jobs_running -= 1
                    if jobs_running == 0:
                        job_status = "COMPLETE"
                if event.type == htcondor.JobEventType.JOB_HELD or event.type == htcondor.JobEventType.EXECUTABLE_ERROR:
                    job_status = "ERROR"

            # Now that we have all the information we want, display it
            current_time = datetime.now()
            time_diff = None
            if job_status is "PROVISIONING REQUEST PENDING":
                time_diff = current_time - provisioner_job_started_time
            elif job_status is "RUNNING":
                time_diff = current_time - job_started_time

            print(f"Job is {job_status}", end='')
            if time_diff is not None:
                print(f" since {round(time_diff.seconds/60)}m{(time_diff.seconds%60)}s")
            else:
                print("")


    @staticmethod
    def resources(id, options=None):
        """
        Displays the resources used by a specified job
        """

        # If no resource specified, assume job is running on local pool
        if "resource" not in options:
            try:
                job = schedd.query(
                    constraint=f"ClusterId == {id}",
                    projection=["RemoteHost"]
                )
                job_host = JobStatus[job[0]["RemoteHost"]]
                print(f"Job is using resource {job_host}")
            except IndexError:
                print(f"No jobs found for cluster ID {id}.")
                sys.exit(0)
            except:
                print(f"Error looking up job status: {sys.exc_info()[0]}")
                sys.exit(1)

        # Jobs running on provisioned Slurm resources need to retrieve
        # additional information from the provisioning DAGMan log
        elif options["resource"] == "slurm":

            # Internal variables
            dagman_cluster_id = None
            provisioner_cluster_id = None
            slurm_cluster_id = None
        
            # User-facing variables (all values set below are default/initial state)
            provisioner_job_started_time = None
            provisioner_job_scheduled_end_time = None
            job_status = "NOT SUBMITTED"
            job_started_time = None
            jobs_running = 0
            slurm_nodes_requested = None
            slurm_runtime = None

            dagman_dag, dagman_out, dagman_log = DAGMan.get_files(id)

            if dagman_dag is None:
                print(f"No Slurm jobs found for cluster ID {id}.")
                sys.exit(0)

            # Parse the .dag file to retrieve some user input values
            dagman_dag_file = open(dagman_dag, "r")
            for line in dagman_dag_file.readlines():
                if "annex_node_count =" in line:
                    slurm_nodes_requested = line.split("=")[1].strip()
                if "annex_runtime =" in line:
                    slurm_runtime = int(line.split("=")[1].strip())

            # Parse the DAGMan event log for useful information
            dagman_events = htcondor.JobEventLog(dagman_log)
            for event in dagman_events.events(0):
                if "LogNotes" in event.keys() and event["LogNotes"] == "DAG Node: B":
                    provisioner_cluster_id = event.cluster
                    provisioner_job_started_time = datetime.fromtimestamp(event.timestamp)
                    provisioner_job_scheduled_end_time = datetime.fromtimestamp(event.timestamp + slurm_runtime)
                    job_status = "PROVISIONING REQUEST PENDING"
                if "LogNotes" in event.keys() and event["LogNotes"] == "DAG Node: C":
                    slurm_cluster_id = event.cluster
                    if job_started_time is None:
                        job_started_time = datetime.fromtimestamp(event.timestamp)
                if event.cluster == slurm_cluster_id and event.type == htcondor.JobEventType.EXECUTE:
                    job_status = "RUNNING"
                    jobs_running += 1
                if event.cluster == slurm_cluster_id and (event.type == htcondor.JobEventType.JOB_TERMINATED or event.type == htcondor.JobEventType.JOB_EVICTED):
                    jobs_running -= 1
                    if jobs_running == 0:
                        job_status = "COMPLETE"
                if event.type == htcondor.JobEventType.JOB_HELD or event.type == htcondor.JobEventType.EXECUTABLE_ERROR:
                    job_status = "ERROR"

            # Now that we have all the information we want, display it
            if job_status is "PROVISIONING REQUEST PENDING":
                print(f"Job is still waiting for {slurm_nodes_requested} Slurm nodes to provision")
            elif job_status is "RUNNING":
                print(f"Job is running on {jobs_running}/{slurm_nodes_requested} requested Slurm nodes")
            elif job_status is "ERROR":
                print(f"An error occurred provisioning Slurm resources")

            # Show information about time remaining
            if job_status is "RUNNING" or job_status is "COMPLETE":
                current_time = datetime.now()
                if current_time < provisioner_job_scheduled_end_time:
                    time_diff = provisioner_job_scheduled_end_time - current_time
                    print(f"Slurm resources are reserved for another {round(time_diff.seconds/60)}m{(time_diff.seconds%60)}s")
                else:
                    time_diff = current_time - provisioner_job_scheduled_end_time
                    print(f"Slurm resources were terminated since {round(time_diff.seconds/60)}m{(time_diff.seconds%60)}s")


def main():

    # Parse arguments and set default values if undeclared
    try:
        args = parse_args()
    except Exception as err:
        print(f"Failed to parse arguments: {err}", file=sys.stderr)

    # Assume any error checking on the inputs happens during parsing
    command = args["command"]
    options = args["options"]

    # Set up some globally-needed things
    global schedd
    try:
        schedd = htcondor.Schedd()
    except:
        print(f"Could not access local schedd. This tool must be a run from a submit machine, or specify a remote submit host with the --schedd=schedd.hostname flag.")
        sys.exit(1)

    # Figure out what the user is asking for and show it
    if command["object"] == "job":
        if command["action"] == "submit":
            Job.submit(command["target"], options)
        if command["action"] == "status":
            Job.status(command["target"], options)
        elif command["action"] == "resources":
            Job.resources(command["target"], options)
        else:
            print(f"Error: The '{command['action']}' action is not supported for job objects")
    else:
        print(f"Error: '{command['object']}' is not a valid command object") 

    # All done
    sys.exit(0)


if __name__ == "__main__":
    main()
